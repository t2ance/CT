# Configuration for CT Super-Resolution with Latent Diffusion
# Model: 3D UNet Diffusion in VQ-AE Latent Space
# Last updated: 2025-01-12 (Fixed data_range bug)

# Data configuration
data:
  # Path to cached latent representations (pre-encoded by VQ-AE)
  latent_cache_dir: './latents_cache'

  # VQ-AE checkpoint for decoding (visualization & metrics)
  vae_checkpoint: '/data2/peijia/projects/BioAgent/3D-MedDiffusion/checkpoints/PatchVolume_8x_s2.ckpt'

  # Latent normalization (keep False - latents are ~[-30, 30])
  normalize_latents: false

  # Expected latent shape (after VQ-AE encoding)
  # Example: [200, 128, 128] CT → [25, 16, 16] latent (8× compression)
  latent_channels: 8  # Cℓ from VQ-AE

# Model architecture (3D UNet for diffusion)
model:
  # Base model channels (scaled up from 64 for better performance)
  model_channels: 128  # Was 64 (SCALED UP)

  # Channel multipliers per resolution level
  channel_mult: [1, 2, 4, 8]  # Was [1, 2, 4, 4] (SCALED UP)

  # Number of ResNet blocks per level
  num_blocks: 3  # Was 2 (SCALED UP)

  # Which levels get attention (0=highest res, 3=lowest res)
  attention_levels: [1, 2, 3]  # Was [2, 3] (MORE ATTENTION)

  # Time embedding dimension
  time_embed_dim: 512  # Was 256 (SCALED UP)

  # Dropout for regularization
  dropout: 0.1  # Was 0.0 (ADDED DROPOUT)

  # Number of attention heads
  num_heads: 8

  # Input/output channels (latent space)
  in_channels: 8  # LDCT latent
  out_channels: 8  # HDCT latent (predict noise in latent space)

  # Conditioning: concatenate LDCT latent to HDCT latent
  concat_conditioning: true

# Diffusion scheduler
scheduler:
  # Training timesteps (T in DDPM paper)
  num_train_timesteps: 1000

  # Beta schedule for noise
  beta_schedule: 'linear'  # 'linear' or 'scaled_linear'
  beta_start: 0.0001
  beta_end: 0.02

  # Prediction type
  prediction_type: 'epsilon'  # Predict noise (standard DDPM)

  # Don't clip samples (latent space is ~[-30, 30])
  clip_sample: false

# Training configuration
training:
  # Batch size per GPU (FIXED from 512)
  batch_size: 32  # Was 512 (CRITICAL FIX for diffusion)

  # Gradient accumulation (effective batch = 32 * 2 = 64)
  gradient_accumulation_steps: 2  # Was 1

  # Number of training epochs
  num_epochs: 50

  # Optimizer
  learning_rate: 2.0e-4  # Was 5e-5 (INCREASED)
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  weight_decay: 0.01

  # Learning rate scheduler
  lr_scheduler_type: 'linear'  # 'linear' with warmup
  warmup_ratio: 0.06  # 6% of total steps for warmup
  lr_min: 0.0  # Minimum LR after decay

  # Mixed precision training
  mixed_precision: 'fp16'  # 'fp16', 'bf16', or 'no'

  # Gradient clipping
  max_grad_norm: 1.0

  # Validation and checkpointing
  val_every: 50  # Lightweight validation (MSE only)
  save_every: 200  # Save checkpoint every N steps

  # Number of dataloader workers
  num_workers: 4

  # Logging
  log_every: 10  # Log metrics every N steps
  use_wandb: true
  wandb_project: 'ct-latent-diffusion'
  wandb_entity: null  # Set to your wandb username
  wandb_run_name: null  # Auto-generated if null
  wandb_tags: ['latent-diffusion', '3d-ct', 'vq-ae']

# Validation configuration (for SSIM/PSNR metrics)
validation:
  # Two-tier validation strategy:
  # - Lightweight: Only MSE loss (fast, every val_every steps)
  # - Full: SSIM + PSNR metrics (slow, every full_val_every steps)

  # Full validation settings
  enable_full_validation: true  # Enable SSIM/PSNR metrics
  full_val_every: 500  # Run full validation every N steps (slower)
  num_samples: 32  # Number of validation samples to evaluate (4-8 recommended)

  # Metrics settings
  data_range: 2.0  # VQ-AE output range for SSIM/PSNR (normalized to ~[-1, 1])
                   # FIXED: Was 2000.0 (caused inflated metrics!)
  compute_slice_wise: true  # Compute metrics slice-wise (more robust for 3D)

  # Distributed evaluation
  distributed_eval: true  # Enable distributed evaluation across GPUs

# Inference configuration
inference:
  # DDIM sampling (faster than DDPM)
  use_ddim: true
  num_inference_steps: 100  # Was 50 (can increase for quality)

  # Guidance scale (for classifier-free guidance, if implemented)
  guidance_scale: 1.0  # 1.0 = no guidance

# Output directories
output:
  # Where to save checkpoints and logs
  checkpoint_dir: './outputs/checkpoints'
  log_dir: './outputs/logs'
  visualization_dir: './outputs/visualizations'

# Distributed training (accelerate)
accelerate:
  # See accelerate config for multi-GPU settings
  # Run: accelerate config
  # Or use: accelerate launch --multi_gpu --num_processes=2 train_diffusion.py
  pass

# Notes:
# 1. This config uses latent diffusion (two-stage approach)
# 2. VQ-AE is frozen and used only for encoding/decoding
# 3. Diffusion happens in compressed latent space (8× faster than pixel space)
# 4. Data range fixed: 2.0 for normalized VQ-AE outputs (not 2000.0 for HU values)
# 5. Model scaled up for better performance (128 channels, deeper hierarchy)
